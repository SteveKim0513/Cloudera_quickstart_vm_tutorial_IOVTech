<html>

 <head>

 </head>

  <body>

<p class="1" style="line-height: 160%; mso-pagination: none; mso-padding-alt: 0.0pt 0.0pt 0.0pt 0.0pt;"><span lang="EN-US" style="color: rgb(49, 95, 151); letter-spacing: 0pt; font-family: «—ƒƒπŸ≈¡; font-size: 20pt; mso-ascii-font-family: Century Gothic; mso-font-width: 100%; mso-text-raise: 0.0pt;">Tutorial Exercise 1 </span></p>
<p class="1" style="line-height: 160%; mso-pagination: none; mso-padding-alt: 0.0pt 0.0pt 0.0pt 0.0pt;">  &nbsp;  

<o:p></o:p></p><p class="1" style="line-height: 160%; mso-pagination: none; mso-padding-alt: 0.0pt 0.0pt 0.0pt 0.0pt;">  &nbsp; <o:p></o:p></p><p class="1" style="line-height: 160%; mso-pagination: none; mso-padding-alt: 0.0pt 0.0pt 0.0pt 0.0pt;"><span lang="EN-US" style="color: rgb(233, 174, 43); letter-spacing: 0pt; font-family: «—ƒƒπŸ≈¡; font-size: 15pt; mso-ascii-font-family: Century Gothic; mso-font-width: 100%; mso-text-raise: 0.0pt;">Ingest Structured Data </span></p>

<p class="1" style="line-height: 160%; mso-pagination: none; mso-padding-alt: 0.0pt 0.0pt 0.0pt 0.0pt;"><span lang="EN-US" style="letter-spacing: 0pt; font-family: «—ƒƒπŸ≈¡; mso-ascii-font-family: Century Gothic; mso-font-width: 100%; mso-text-raise: 0.0pt;">In this scenario, DataCo°Øs business question is: What products do our customers like to buy? To answer this question, the first thought might be to look at the transaction data, which should indicate what customers actually do buy and like to buy, right? </span></p>

<p class="1" style="line-height: 160%; mso-pagination: none; mso-padding-alt: 0.0pt 0.0pt 0.0pt 0.0pt;"><span lang="EN-US" style="letter-spacing: 0pt; font-family: «—ƒƒπŸ≈¡; mso-ascii-font-family: Century Gothic; mso-font-width: 100%; mso-text-raise: 0.0pt;">This is probably something you can do in your regular RDBMS environment, but a benefit with Cloudera°Øs platform is that you can do it at greater scale at lower cost, on the same system that you may also use for many other types of analysis. </span></p>

<p class="1" style="line-height: 160%; mso-pagination: none; mso-padding-alt: 0.0pt 0.0pt 0.0pt 0.0pt;">  &nbsp; <o:p></o:p></p><p class="1" style="line-height: 160%; mso-pagination: none; mso-padding-alt: 0.0pt 0.0pt 0.0pt 0.0pt;"><span lang="EN-US" style="letter-spacing: 0pt; font-family: «—ƒƒπŸ≈¡; mso-ascii-font-family: Century Gothic; mso-font-width: 100%; mso-text-raise: 0.0pt;">What this exercise demonstrates is how to do exactly the same thing you already know how to do, but in CDH. Seamless integration is important when evaluating any new infrastructure. Hence, it°Øs important to be able to do what you normally do, and not break any regular BI reports or workloads over the dataset you plan to migrate. </span></p>

<p class="0" style="mso-pagination: none; mso-padding-alt: 0.0pt 0.0pt 0.0pt 0.0pt;">  &nbsp;<o:p></o:p></p>

<p><br></p>


image




<p class="1" style="line-height: 160%; mso-pagination: none; mso-padding-alt: 0.0pt 0.0pt 0.0pt 0.0pt;"><span lang="EN-US" style="letter-spacing: 0pt; font-family: «—ƒƒπŸ≈¡; mso-ascii-font-family: Century Gothic; mso-font-width: 100%; mso-text-raise: 0.0pt;">

To analyze the transaction data in the new platform, we need to ingest it into the Hadoop Distributed File System (HDFS). We need to find a tool that easily transfers structured data from a RDBMS to HDFS, while preserving structure. That enables us to query the data, but not interfere with or break any regular workload on it. 
Apache Sqoop, which is part of CDH, is that tool. The nice thing about Sqoop is that we can automatically load our relational data from MySQL into HDFS, while preserving the structure. 
With a few additional configuration parameters, we can take this one step further and load this relational data directly into a form ready to be queried by Impala (the open source analytic query engine included with CDH). Given that we may want to leverage the power of the Apache Avro file format for other workloads on the cluster (as Avro is a Hadoop optimized file format), we will take a few extra steps to load this data into Impala using the Avro file format, so it is readily available for Impala as well as other workloads. 
You should first log in to the Master Node of your cluster using SSH - you can get the credentials using the instructions on Your Cloudera Cluster. Once you are logged in, you can launch the Sqoop job: 
You should first open a terminal, which you can do by clicking the black "Terminal" icon at the top of your screen or clicking right button of mouse to see the menu.  </span></p>


<p class="1" style="text-align: justify; line-height: 160%; mso-pagination: none; mso-padding-alt: 0.0pt 0.0pt 0.0pt 0.0pt;"><span lang="EN-US" style="letter-spacing: 0pt; font-family: «—ƒƒπŸ≈¡; mso-ascii-font-family: Century Gothic; mso-font-width: 100%; mso-text-raise: 0.0pt;">And input the command like this :</span></p>

<table style="border: 0.28pt solid rgb(0, 0, 0); border-image: none; border-collapse: collapse; mso-table-overlap: never;">
<tr>
<td valign="top" style="background: rgb(242, 242, 242); padding: 1.41pt 5.1pt; border: 0.28pt solid rgb(0, 0, 0); border-image: none; width: 663.31pt; height: 130.93pt;">

[root@quickstart.cloudera ~] sqoop import-all-tables  \ <br>
-m 1  \ <br>
--connect jdbc:mysql://quickstart.cloudera:3306/retail_db  \ <br>
--username=retail_dba  \ <br>
--password=cloudera  \ <br>
--compression-codec=snappy  \ <br>
--as-avrodatafile  \ <br>
--warehouse-dir=/user/hive/warehouse <br>

</td>
</tr></table>

<br><br>



<table style="border: 0.28pt solid rgb(0, 0, 0); border-image: none; border-collapse: collapse; mso-table-overlap: never;">
<tr>
<td valign="top" style="background: rgb(242, 242, 242); padding: 1.41pt 5.1pt; border: 0.28pt solid rgb(0, 0, 0); border-image: none; width: 663.31pt; height: 346.97pt;">
[root@quickstart.cloudera ~] sqoop import-all-tables  \ <br>
®Á<u style="text-underline: #ff0000 double; color=red">
-m 1</u> \
--connect
®Ë<u style="text-underline: #ff0000 double;"> jdbc
</u>
:mysql://quickstart.cloudera:3306/retail_db  \  <br>
--username=retail_dba  \ <br>
--password=cloudera  \ <br>
®È<u style="text-underline: #ff0000 double;"> --compression-codec=snappy \
</u>
--as-
®Í<u style="text-underline: #ff0000 double;"> avro </u> datafile \ <br>
--warehouse-dir=/user/hive/warehouse  <br><br><br>

®Á -m <n> : It is a command option to use n map tasks to import in parallel.<br>

®Ë JDBC is a Java database connectivity technology (Java Standard Edition platform) from Oracle Corporation. This technology is an API for the Java programming language that defines how a client may access a database. It provides methods for querying and updating data in a database. JDBC is oriented towards relational databases.<br>

®È On the command line, use the following option to enable Snappy compression.<br>
cf. Snappy is a compression/decompression library. It does not aim for maximum compression, or compatibility with any other compression library; instead, it aims for very high speeds and reasonable compression. For instance, compared to the fastest mode of zlib, Snappy is an order of magnitude faster for most inputs, but the resulting compressed files are anywhere from 20% to 100% bigger. On a single core of a Core i7 processor in 64-bit mode, Snappy compresses at about 250 MB/sec or more and decompresses at about 500 MB/sec or more.<br>
®Í Avro is a data serialization system.
</td>
</tr></table>

<p><br></p>


<p class="1" style="line-height: 160%; mso-pagination: none; mso-padding-alt: 0.0pt 0.0pt 0.0pt 0.0pt;">  <o:p></o:p></p>

<p class="1" style="line-height: 160%; mso-pagination: none; mso-padding-alt: 0.0pt 0.0pt 0.0pt 0.0pt;"><span lang="EN-US" style="letter-spacing: 0pt; font-family: «—ƒƒπŸ≈¡; mso-ascii-font-family: Century Gothic; mso-font-width: 100%; mso-text-raise: 0.0pt;">This command may take a while to complete, but it is doing a lot. It is launching MapReduce jobs to export the data from our MySQL database, and put those export files in Avro format in HDFS. It is also creating the Avro schema, so that we can easily load our Hive tables for use in Impala later.</span></p>

<p class="1" style="line-height: 160%; mso-pagination: none; mso-padding-alt: 0.0pt 0.0pt 0.0pt 0.0pt;">   <o:p></o:p></p>

<p class="1" style="line-height: 160%; mso-pagination: none; mso-padding-alt: 0.0pt 0.0pt 0.0pt 0.0pt;">   <o:p></o:p></p>

<p class="1" style="line-height: 160%; mso-pagination: none; mso-padding-alt: 0.0pt 0.0pt 0.0pt 0.0pt;"> 

<p class="1" style="line-height: 160%; mso-pagination: none; mso-padding-alt: 0.0pt 0.0pt 0.0pt 0.0pt;"><span lang="EN-US" style="letter-spacing: 0pt; font-family: «—ƒƒπŸ≈¡; font-size: 15pt; mso-ascii-font-family: Century Gothic; mso-font-width: 100%; mso-text-raise: 0.0pt;">Verification step : </span></p>

<p class="1" style="line-height: 160%; mso-pagination: none; mso-padding-alt: 0.0pt 0.0pt 0.0pt 0.0pt;"><span lang="EN-US" style="letter-spacing: 0pt; font-family: «—ƒƒπŸ≈¡; mso-ascii-font-family: Century Gothic; mso-font-width: 100%; mso-text-raise: 0.0pt;">When this command is complete, confirm that your Avro data files exist in HDFS. </span></p>

<table style="border: 0.28pt solid rgb(0, 0, 0); border-image: none; border-collapse: collapse; mso-table-overlap: never;">

<tr><td valign="top" style="background: rgb(242, 242, 242); padding: 1.41pt 5.1pt; border: 0.28pt solid rgb(0, 0, 0); border-image: none; width: 663.31pt; height: 34.71pt;">


[root@quickstart.cloudera ~] hadoop fs -ls /user/hive/warehouse


</td></tr>
</table>

<br><br>

<table style="border: 0.28pt solid rgb(0, 0, 0); border-image: none; border-collapse: collapse; mso-table-overlap: never;">

<tr><td valign="top" style="background: rgb(242, 242, 242); padding: 1.41pt 5.1pt; border: 0.28pt solid rgb(0, 0, 0); border-image: none; width: 663.31pt; height: 69.63pt;">


[comments] <br>
[root@quickstart.cloudera ~] hadoop fs ®Á <u style="text-underline: #ff0000 double;"> -ls</u>/user/hive/warehouse <br>
®Á -ls : is one of the most frequently used command in Linux. It shows lists files and directories.

</td></tr>
</table>

<p class="1" style="line-height: 160%; mso-pagination: none; mso-padding-alt: 0.0pt 0.0pt 0.0pt 0.0pt;"><span lang="EN-US" style="letter-spacing: 0pt; font-family: «—ƒƒπŸ≈¡; mso-ascii-font-family: Century Gothic; mso-font-width: 100%; mso-text-raise: 0.0pt;">Will show a folder for each of the tables. </span></p>


<table style="border: 0.28pt solid rgb(0, 0, 0); border-image: none; border-collapse: collapse; mso-table-overlap: never;">

<tr><td valign="top" style="background: rgb(242, 242, 242); padding: 1.41pt 5.1pt; border: 0.28pt solid rgb(0, 0, 0); border-image: none; width: 663.31pt; height: 34.71pt;">

[root@quickstart.cloudera ~] hadoop fs -ls /user/hive/warehouse/categories/

</td></tr></table>

<p class="1" style="line-height: 160%; mso-pagination: none; mso-padding-alt: 0.0pt 0.0pt 0.0pt 0.0pt;"><span lang="EN-US" style="letter-spacing: 0pt; font-family: «—ƒƒπŸ≈¡; mso-ascii-font-family: Century Gothic; mso-font-width: 100%; mso-text-raise: 0.0pt;">Will show the files that live inside of the categories folder. </span></p>

<p class="1" style="line-height: 160%; mso-pagination: none; mso-padding-alt: 0.0pt 0.0pt 0.0pt 0.0pt;"><span lang="EN-US" style="letter-spacing: 0pt; font-family: «—ƒƒπŸ≈¡; mso-ascii-font-family: Century Gothic; mso-font-width: 100%; mso-text-raise: 0.0pt;">Result : </span></p>

<br><br>

image

<br><br>





<p class="1" style="line-height: 160%; mso-pagination: none; mso-padding-alt: 0.0pt 0.0pt 0.0pt 0.0pt;"><span style="letter-spacing: 0pt; font-weight: bold; mso-fareast-font-family: «—ƒƒπŸ≈¡; mso-font-width: 100%; mso-text-raise: 0.0pt;">°ÿ </span><span lang="EN-US" style="letter-spacing: 0pt; font-family: «—ƒƒπŸ≈¡; font-weight: bold; mso-font-width: 100%; mso-text-raise: 0.0pt; mso-ascii-font-family: Century Gothic;">Note : The file lists depend on quickstart version. So you can see the different result page.</span></p><p class="1" style="line-height: 160%; mso-pagination: none; mso-padding-alt: 0.0pt 0.0pt 0.0pt 0.0pt;">  <o:p></o:p></p>







<p class="1" style="line-height: 160%; mso-pagination: none; mso-padding-alt: 0.0pt 0.0pt 0.0pt 0.0pt;"><span style="letter-spacing: 0pt; font-weight: bold; mso-fareast-font-family: «—ƒƒπŸ≈¡; mso-font-width: 100%; mso-text-raise: 0.0pt;">°ÿ </span><span lang="EN-US" style="letter-spacing: 0pt; font-family: «—ƒƒπŸ≈¡; font-weight: bold; mso-font-width: 100%; mso-text-raise: 0.0pt; mso-ascii-font-family: Century Gothic;">Note : The file lists depend on quickstart version. So you can see the different result page.</span></p><p class="1" style="line-height: 160%; mso-pagination: none; mso-padding-alt: 0.0pt 0.0pt 0.0pt 0.0pt;">  <o:p></o:p></p>


<p class="1" style="line-height: 160%; mso-pagination: none; mso-padding-alt: 0.0pt 0.0pt 0.0pt 0.0pt;"><span lang="EN-US" style="letter-spacing: 0pt; font-family: «—ƒƒπŸ≈¡; mso-font-width: 100%; mso-text-raise: 0.0pt; mso-ascii-font-family: Century Gothic;">Sqoop should also have created schema files for this data in your home directory. <br> Avro schema files : </span></p>

<br>

<table style="border: 0.28pt solid rgb(0, 0, 0); border-image: none; border-collapse: collapse; mso-table-overlap: never;">

<tr><td valign="top" style="background: rgb(242, 242, 242); padding: 1.41pt 5.1pt; border: 0.28pt solid rgb(0, 0, 0); border-image: none; width: 663.31pt; height: 34.71pt;">

[root@quickstart.cloudera ~] ls -l*.avsc
</td></tr>

</table>

<br><br>

<table style="border: 0.28pt solid rgb(0, 0, 0); border-image: none; border-collapse: collapse; mso-table-overlap: never;">


<tr><td valign="top" style="background: rgb(242, 242, 242); padding: 1.41pt 5.1pt; border: 0.28pt solid rgb(0, 0, 0); border-image: none; width: 663.31pt; height: 69.63pt;">

[comments]<br>
[root@quickstart.cloudera ~] ls ®Á<u style="text-underline: #ff0000 double;">
-l</u>*.avsc 
<br>
®Á -l : is the option of ls command. It is to show list one file per line.

</td></tr>

</table>

<p class="1" style="line-height: 160%; mso-pagination: none; mso-padding-alt: 0.0pt 0.0pt 0.0pt 0.0pt;"><span lang="EN-US" style="letter-spacing: 0pt; font-family: «—ƒƒπŸ≈¡; mso-font-width: 100%; mso-text-raise: 0.0pt; mso-ascii-font-family: Century Gothic;">Should show .avsc files for the tables that were in our retail_db.
<br>
Result : </span></p>


<font face="Arial" size="5">

<b>Tutorial Exercise 1</b> </font><br><br>

<font face="Arial" size="3">

Note that the schema and the data are stored in separate files. The schema is only applied when the data is queried, a technique called 'schema-on-read'. This gives you the flexibility to query the data with SQL while it's still in a format usable by other systems as well. Whereas a traditional database requires the schema to be defined before entering any data, we have already imported a lot of data and will only now specify how it's structure should be interpreted. 
Apache Hive will need the schema files too, so let's copy them into HDFS where Hive can easily access them. 
<p>


<table style="border: 0.28pt solid rgb(0, 0, 0); border-image: none; border-collapse: collapse; mso-table-overlap: never;">
<tr>
<td valign="top" style="background: rgb(242, 242, 242); padding: 1.41pt 5.1pt; border: 0.28pt solid rgb(0, 0, 0); border-image: none; width: 600pt; height: 20pt;">

[root@quickstart.cloudera ~] sudo -u hdfs hadoop fs -mkdir /user/examples
[root@quickstart.cloudera ~] sudo -u hdfs hadoop fs -chmod +rw /user/examples
[root@quickstart.cloudera ~] hadoop fs -copyFromLocal ~/*.asc /user/examples/

</td>
</tr></table>

<br><br>




<table style="border: 0.28pt solid rgb(0, 0, 0); border-image: none; border-collapse: collapse; mso-table-overlap: never;">
<tr>
<td valign="top" style="background: rgb(242, 242, 242); padding: 1.41pt 5.1pt; border: 0.28pt solid rgb(0, 0, 0); border-image: none; width: 600pt; height: 115pt;">
[comments]

[root@quickstart.cloudera ~] ®Ásudo -u hdfs hadoop fs ®Ë-mkdir /user/examples
[root@quickstart.cloudera ~] sudo -u hdfs hadoop fs ®È-chmod +rw /user/examples
[root@quickstart.cloudera ~] hadoop fs -copyFromLocal ~/*.asc /user/examples/

®Á "sudo" is to do something as a different user(typically as root, the "superuser").
®Ë "mkdir" means making directory.
®È "chmod" is used to change the permissions of files or directories. So as you input the command, you change the permission of "/user/examples" folder to add read permission(r) and write permission(w).</td>
</tr></table>

<p>



Now that we have the data, we can prepare it to be queried. There are two tools for doing SQL-like queries in CDH: Hive and Impala. Hive works by translating SQL queries into MapReduce jobs, so it's best for large batch jobs and applying flexible transformations. Impala is significantly faster and is intended to have low enough latency for interactive queries and data exploration. Impala and Hive can share the metadata about these tables (and their query languages are similar), so we'll use Hive's shell to define these tables, similar to how you would have defined them in your relational database. Hive's query language will look very familiar if you know SQL, and they are intended to be as compatible as possible. There are differences however, as Hive and Impala's abilities introduce some new concepts. Launch the Hive shell and create the tables. 

<p>


<table style="border: 0.28pt solid rgb(0, 0, 0); border-image: none; border-collapse: collapse; mso-table-overlap: never;">
<tr>
<td valign="top" style="background: rgb(242, 242, 242); padding: 1.41pt 5.1pt; border: 0.28pt solid rgb(0, 0, 0); border-image: none; width: 600pt; height: 20pt;">


hive>

</td>
</tr></table>




<p>


<b> Copy and paste the queries below, and hit enter.</b>

<table style="border: 0.28pt solid rgb(0, 0, 0); border-image: none; border-collapse: collapse; mso-table-overlap: never;">
<tr>
<td valign="top" style="background: rgb(242, 242, 242); padding: 1.41pt 5.1pt; border: 0.28pt solid rgb(0, 0, 0); border-image: none; width: 600pt; height: 130.93pt;">
CREATE EXTERNAL TABLE categories
ROW FORMAT SERDE 'org.apache.hadoop.hive.serde2.avro.AvroSerDe'
STORED AS INPUTFORMAT 'org.apache.hadoop.hive.ql.io.avro.AvroContainerInputFormat'
OUTPUTFORMAT 'org.apache.hadoop.hive.ql.io.avro.AvroContainerOutputFormat'
LOCATION 'hdfs:///user/hive/warehouse/categories'
TBLPROPERTIES ('avro.schema.url'='hdfs://quickstart.cloudera/user/examples/sqoop_import_categories.avsc');

CREATE EXTERNAL TABLE customers
ROW FORMAT SERDE 'org.apache.hadoop.hive.serde2.avro.AvroSerDe'
STORED AS INPUTFORMAT 'org.apache.hadoop.hive.ql.io.avro.AvroContainerInputFormat'
OUTPUTFORMAT 'org.apache.hadoop.hive.ql.io.avro.AvroContainerOutputFormat'
LOCATION 'hdfs:///user/hive/warehouse/customers'
TBLPROPERTIES ('avro.schema.url'='hdfs://quickstart.cloudera/user/examples/sqoop_import_customers.avsc');

CREATE EXTERNAL TABLE departments
ROW FORMAT SERDE 'org.apache.hadoop.hive.serde2.avro.AvroSerDe'
STORED AS INPUTFORMAT 'org.apache.hadoop.hive.ql.io.avro.AvroContainerInputFormat'
OUTPUTFORMAT 'org.apache.hadoop.hive.ql.io.avro.AvroContainerOutputFormat'
LOCATION 'hdfs:///user/hive/warehouse/departments'
TBLPROPERTIES ('avro.schema.url'='hdfs://quickstart.cloudera/user/examples/sqoop_import_departments.avsc');

CREATE EXTERNAL TABLE orders
ROW FORMAT SERDE 'org.apache.hadoop.hive.serde2.avro.AvroSerDe'
STORED AS INPUTFORMAT 'org.apache.hadoop.hive.ql.io.avro.AvroContainerInputFormat'
OUTPUTFORMAT 'org.apache.hadoop.hive.ql.io.avro.AvroContainerOutputFormat'
LOCATION 'hdfs:///user/hive/warehouse/orders'
TBLPROPERTIES ('avro.schema.url'='hdfs://quickstart.cloudera/user/examples/sqoop_import_orders.avsc');

CREATE EXTERNAL TABLE order_items
ROW FORMAT SERDE 'org.apache.hadoop.hive.serde2.avro.AvroSerDe'
STORED AS INPUTFORMAT 'org.apache.hadoop.hive.ql.io.avro.AvroContainerInputFormat'
OUTPUTFORMAT 'org.apache.hadoop.hive.ql.io.avro.AvroContainerOutputFormat'
LOCATION 'hdfs:///user/hive/warehouse/order_items'
TBLPROPERTIES ('avro.schema.url'='hdfs://quickstart.cloudera/user/examples/sqoop_import_order_items.avsc'); 

CREATE EXTERNAL TABLE products 
ROW FORMAT SERDE 'org.apache.hadoop.hive.serde2.avro.AvroSerDe' 
STORED AS INPUTFORMAT 'org.apache.hadoop.hive.ql.io.avro.AvroContainerInputFormat' 
OUTPUTFORMAT 'org.apache.hadoop.hive.ql.io.avro.AvroContainerOutputFormat' 
LOCATION 'hdfs:///user/hive/warehouse/products' 
TBLPROPERTIES ('avro.schema.url'='hdfs://quickstart.cloudera/user/examples/sqoop_import_products.avsc'); </td>
</tr></table>




<table style="border: 0.28pt solid rgb(0, 0, 0); border-image: none; border-collapse: collapse; mso-table-overlap: never;">
<tr>
<td valign="top" style="background: rgb(242, 242, 242); padding: 1.41pt 5.1pt; border: 0.28pt solid rgb(0, 0, 0); border-image: none; width: 600pt; height: 400pt;">



[comments]

®ÁCREATE EXTERNAL TABLE categories
®ËROW FORMAT SERDE 'org.apache.hadoop.hive.serde2.avro.AvroSerDe'
STORED AS INPUTFORMAT 'org.apache.hadoop.hive.ql.io.avro.AvroContainerInputFormat'
OUTPUTFORMAT 'org.apache.hadoop.hive.ql.io.avro.AvroContainerOutputFormat'
®ÈLOCATION 'hdfs:///user/hive/warehouse/categories'
®Í TBLPROPERTIES ('avro.schema.url'='hdfs://quickstart.cloudera/user/examples/sqoop_import_categories.avsc');





</td>
</tr></table>




Now you should have the following tables created. <br>


<br><br>












  </body>


</html>